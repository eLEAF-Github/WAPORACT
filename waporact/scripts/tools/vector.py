"""
waporact package

vector functions (stand alone/support functions)
"""
##########################
# import packages
import os
from typing import Union
import sys
from datetime import timedelta
from timeit import default_timer

from osgeo import ogr
from osgeo import osr
from osgeo import gdal

import pandas as pd
import geopandas as gpd
import numpy as np
import fiona
from shapely.geometry import mapping, shape, MultiPolygon, Polygon
import shapely.ops as ops
import rtree
import pyproj
from functools import partial

from waporact.scripts.tools.raster import gdal_info
from waporact.scripts.tools.statistics import dict_to_dataframe

##########################
def file_to_records(
    table: Union[str, pd.DataFrame],
    column_mapping: dict = None,
    default_values: dict = None,
    sep: str = ';',
    filter: dict = None,
    output_crs=None,
    sheet=0,
    to_dict: bool=False) -> dict:
    """
    Description:
        reads in a file (shapefile, excel, csv, json, dataframe) and
        extracts them to  according to the specifications as
        a dataframe or dict. If dict it filters the rows (values)
        out and stores them as dicts according to
        key (column name), value (column, row item)

        NOTE: if a dataframe is passed the whole process is skipped
        and the dataframe is outputted again

        NOTE: filter only works for dict at this time

    Args:
        table: path to the file of interest
        column_mapping: mapping for the columns to dict keys
        default values: default values to use for keys if no
        value is found
        sep: sep to use if reading from csv
        filter: dict of a key (column) and value to filter the
        retrieved dicts by
        output_crs: output crs if retrieving from shapefile
        sheet: sheet to use if retrieving from excel
        to_dict: if True returns a list of dicts per geom/record

    Return:
        list, dataframe, geodataframe : extracted records mapped from the file
    """
    if isinstance(table, pd.DataFrame):
        records = table

    else:
        ext = os.path.splitext(table)[1]

        if ext == '.shp':
            df = gpd.read_file(table)
            if output_crs:
                df = df.to_crs('EPSG:{}'.format(output_crs))
            
            df['st_aswkt'] = df.geometry.to_wkt()

        elif ext == '.xlsx':
            df = pd.read_excel(table,sheet_name=sheet)

        elif ext == '.csv':
            df = pd.read_csv(table,sep=sep)

        elif ext == '.json':
            df = pd.read_json(table)

        else:
            raise AttributeError('either a shapefile, csv, excel or json needs to be provided')

        if to_dict:
            records = df.to_dict('records')

            if column_mapping:
                mapped_records = []
                for rec in records:
                    new_rec = rec.copy()
                    for k_new, k_old in column_mapping.items():
                        new_rec = {k_new if k == k_old else k:v for k,v in new_rec.items()}
                    mapped_records.append(new_rec)

                records = mapped_records

            if default_values:
                default_records = []
                for rec in records:
                    for k, v in default_values.items():
                        rec.setdefault(k, v)
                    default_records.append(rec)

                records = default_records

            if filter:
                filtered_records = []
                for rec in records:
                    if any(rec[k] in v for k, v in filter.items()):
                        filtered_records.append(rec)
                    
                records = filtered_records
        
        else:
            records = df

    return records

##########################
def records_to_shapefile(
    records: Union[dict, pd.DataFrame],
    output_shapefile_path: str,
    fields_shapefile_path: str,
    union_key: str,
    output_crs: int = None):
    """
    Description:
        takes a a dataframe or dict and outputs it to a shapefile
        by attaching it to a empty version of the given fields shapefile.

    Args:
        records: records to attach
        output_shapefile_path: path to output the shapefile too, a geopackage is also made
        as shapefiels cannot have columns with names longer than a few characters.
        fields_shapefile_path: path to the shapefile containing the shapes
        union_key: key to use to combine the geodataframe retrieved from the
        shapefile and the records, 'wpid' the autogenerated key column
        made during wapor analysis is suggested
        output_crs: if given transfroms the data to match this crs if it
        does not yet

    Return:
        int: 0
    """
    gdf = gpd.read_file(fields_shapefile_path)

    # drop unneeded columns
    drop_columns = [col for col in list(gdf.columns) if col not in ['geom','geometry', union_key]]
    gdf.drop(columns=drop_columns)

    # if dict format to dataframe
    if isinstance(records, dict):
        records = dict_to_dataframe(records)
    
    # merge gdf and df
    out_gdf  = gdf.merge(records, on=union_key, how='left')

    geodataframe_to_shapefile(
        geodataframe=out_gdf,
        output_shapefile_path=output_shapefile_path,
        output_crs=output_crs,
    )

    return output_shapefile_path
    
##########################
def geodataframe_to_shapefile(
    geodataframe: gpd.GeoDataFrame, 
    output_shapefile_path: str,
    output_crs: int = None):
    """
    Description:
        takes a geodataframe and outputs it to shapefile

    Args:
        geodataframe: geodataframe to output
        output_file_path: path to output the file too
        output_crs: if given transfroms the data to match this crs if it
        does not yet

    Return:
        int: 0
    """
    # create output subfolders as needed
    output_dir = os.path.dirname(output_shapefile_path) 
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    if output_crs:
        geodataframe = geodataframe.to_crs({'init': 'epsg:{}'.format(output_crs)})

    geodataframe.to_file(output_shapefile_path)

    print("geodataframe outputted to shapefile: {}".format(output_shapefile_path))

    return output_shapefile_path

##########################
def retrieve_geodataframe_bbox(geodataframe: gpd.GeoDataFrame):
    """
    Description:
        retrieves the boundingbox of all geometries in a geodataframe
        increase size by 0.01 total diameter to make sure data falls inside
        when retrieving data from wapor (not cut off by cell size)

    Args:
        geodataframe: geodataframe to retrieve boundingbox for
    
    Return:
        tuple: bbox tuple
    """
    # remove empty geometry
    valid_geom = geodataframe[geodataframe.geometry.map(lambda z: True if not z.is_empty else False)]

    # explode (extract polygons from) multipolygons
    singlepart_geoms = valid_geom.geometry.apply(lambda geom: list(geom) if isinstance(geom, MultiPolygon) else geom).explode()
    
    # get the bounds of each geometry
    bboxes = singlepart_geoms.map(lambda z: z.exterior.xy)

    minx = min([min(box[0]) for box in bboxes])
    miny = min([min(box[1]) for box in bboxes])
    maxx = max([max(box[0]) for box in bboxes])
    maxy = max([max(box[1]) for box in bboxes])

    # increase size by 0.01 total diameter to make sure data falls inside
    # when retrieved

    diffx = maxx - minx 
    diffy = maxy - miny

    addx = 0.01 * diffx
    addy = 0.01 * diffy

    minx = minx - addx 
    maxx = maxx + addx
    miny = miny - addy 
    maxy = maxy + addy  

    return (minx, miny, maxx, maxy)

##########################
def retrieve_geodataframe_central_coords(geodataframe: gpd.GeoDataFrame):
    """
    Description:
        retrieves the boundingbox of all geometries in a geodataframe

    Args:
        geodataframe: geodataframe to retrieve boundingbox for
    
    Return:
        tuple: bbox tuple
    """
    # remove empty geometry
    valid_geom = geodataframe[geodataframe.geometry.map(lambda z: True if not z.is_empty else False)]

    # explode (extract polygons from) multipolygons
    singlepart_geoms = valid_geom.geometry.apply(lambda geom: list(geom) if isinstance(geom, MultiPolygon) else geom).explode()
    
    # get the bounds of each geometry
    bboxes = singlepart_geoms.map(lambda z: z.exterior.xy)

    minx = min([min(box[0]) for box in bboxes])
    miny = min([min(box[1]) for box in bboxes])
    maxx = max([max(box[0]) for box in bboxes])
    maxy = max([max(box[1]) for box in bboxes])

    x = minx + (maxx - minx) / 2
    y = miny + (maxy - miny) / 2

    return x,y

############################
def get_plotting_zoom_level_and_central_coords_from_gdf(
        gdf: gpd.GeoDataFrame):
    """
    Description:

    NOTE: linear sclar interpolation for the zoom levle is taken from
        https://community.plotly.com/t/dynamic-zoom-for-mapbox/32658/7

    Args:
        gdf: geodataframe to mine for the required information

    Return:
        tuple: zoom level and the associated boundary-box center coordinates 
    """
    bbox = retrieve_geodataframe_bbox(geodataframe=gdf)
    bbox_poly = create_bbox_polygon(bbox=bbox)
    area = calc_lat_lon_polygon_area(bbox_poly) /10000

    interp_dict = {
        20: 0,
        19: 100,
        17: 1000,
        14: 10000,
        12: 50000,
        10: 500000,
        9: 1000000,
        7: 5000000,
        5: 5**10,
        3: 6*10,
        1: 8**10,
        }
    zooms = [key for key in interp_dict.keys()]
    areas = [value for value in interp_dict.values()]

    zoom = int(np.interp(x=area,
                     xp=areas,
                     fp=zooms))

    # retrieve central mapping points
    x, y = retrieve_geodataframe_central_coords(gdf)

    # Finally, return the zoom level and the associated boundary-box center coordinates
    return zoom, (x, y)

############################
def retrieve_shapefile_crs(shapefile_path: str) -> int:
    """
    Description:
        retrieve the crs code/number of the given shapefile

    Args:
        shapefile_path: path to the shapefile

    Return:
        int: crs code/number of the input shapefile
    """
    driver = ogr.GetDriverByName('ESRI Shapefile')
    datasource = driver.Open(shapefile_path)
    layer = datasource.GetLayer()

    # retrieve spatialref
    srs = layer.GetSpatialRef()
    crs = int(srs.GetAttrValue("AUTHORITY", 1))

    return crs

############################
def compare_raster_vector_crs(
    raster_path: str, 
    shapefile_path: str):
    """
    Description:
        retrieve the crs code/number of the given shapefile and of the given raster
        and compare them. if they dont match raise an error

    Args:
        shapefile_path: path to the shapefile
        raster_path: path to the raster

    Return:
        int: 0

    Raise:
        AttributeError: if the crs values do not match
    """
    shape_crs = retrieve_shapefile_crs(shapefile_path)
    raster_crs = gdal_info(raster_path)['crs']

    if raster_crs != shape_crs:
        raise AttributeError('crs of the given shapefile: ({},{}), and the given raster: ({}, {}), do not match'.format(
            shapefile_path, shape_crs, raster_path, raster_crs))

    return 0

############################
def shape_reprojection(
    shapefile_path: str,
    output_directory: str,
    crs: int,
    output_name: str='') -> str:
    """
    Description:
        reproject the input shapefile to a different coordinate system using a given crs code

    Args:
        shapefile_path: path to the shapefile
        output_directory: folder to output too
        crs: code/ number of the crs (coordinate reference system)
        output_name: name for the output shapefile if not given the
        original name is appended with '{_reproject'

    Return:
        str: the path to the reprojected shapefile
    """
    if output_name == '':
        output_name = os.path.splitext(os.path.basename(shapefile_path))[0]

    output_shp = os.path.join(output_directory, "{}_{}_reproject.shp".format(output_name, crs))

    if not os.path.exists(output_shp):
            
        # set output SpatialReference
        outSpatialRef = osr.SpatialReference()
        outSpatialRef.ImportFromEPSG(int(crs))

        # input SpatialReference
        inSpatialRef = osr.SpatialReference()
        inSpatialRef.ImportFromEPSG(retrieve_shapefile_crs(shapefile_path=shapefile_path))

        driver = ogr.GetDriverByName('ESRI Shapefile')

        # create the CoordinateTransformation
        coordTrans = osr.CoordinateTransformation(inSpatialRef, outSpatialRef)

        # get the input layer
        inDataSet = driver.Open(shapefile_path)
        inLayer = inDataSet.GetLayer()

        # create the output layer
        outDataSet = driver.CreateDataSource(output_shp)
        outLayer = outDataSet.CreateLayer(output_name, outSpatialRef, geom_type=ogr.wkbMultiPolygon)

        # add fields
        inLayerDefn = inLayer.GetLayerDefn()
        for i in range(0, inLayerDefn.GetFieldCount()):
            fieldDefn = inLayerDefn.GetFieldDefn(i)
            outLayer.CreateField(fieldDefn)

        # get the output layer's feature definition
        outLayerDefn = outLayer.GetLayerDefn()

        # loop through the input features
        inFeature = inLayer.GetNextFeature()
        while inFeature:
            # get the input geometry
            geom = inFeature.GetGeometryRef()
            # reproject the geometry
            geom.Transform(coordTrans)
            # create a new feature
            outFeature = ogr.Feature(outLayerDefn)
            # set the geometry and attribute
            outFeature.SetGeometry(geom)
            for i in range(0, outLayerDefn.GetFieldCount()):
                outFeature.SetField(outLayerDefn.GetFieldDefn(i).GetNameRef(), inFeature.GetField(i))
            # add the feature to the shapefile
            outLayer.CreateFeature(outFeature)
            # dereference the features and get the next input feature
            outFeature = None
            inFeature = inLayer.GetNextFeature()

        # Save and close the shapefiles
        inDataSet = None
        outDataSet = None

    else:
        pass

    return output_shp

############################
def create_bbox_polygon(bbox: tuple):
    """
    Description:
        create a bbox polygon from a tuple of coordinates

    Args:
        bbox: tuple of coordinates

    Return:
        polygon: created polygon
    """
    #set bbox coordinates
    xy_list = [(bbox[0],bbox[3]),
    (bbox[2],bbox[3]),
    (bbox[2],bbox[1]),
    (bbox[0],bbox[1]),]

    # set to polygon
    poly = shape({'type':'Polygon',
        'coordinates': [xy_list]})

    return poly

############################
def create_bbox_shapefile(
    output_shape_path: str,  
    bbox: tuple,
    crs:int=4326):
    """
    Description:
        create a bbox shapefile based on the vectors values in a bbox tuple
        and outputs it to the given location
    
    Args:
        output_shape_path: path to output the created shapefile too
        bbox: tuple of coordinates used to make the polyon and shapefile
        crs: crs (epsg code) for the output shapefile

    Return:
        int: 0 
    """
    # create output subfolders as needed
    output_dir = os.path.dirname(output_shape_path) 
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    #define schema
    schema = {
        'geometry':'Polygon',
        'properties':[('Name','str')]
        }

    #open a fiona object
    shp = fiona.open(output_shape_path, mode='w', driver='ESRI Shapefile',
        schema = schema, crs = "EPSG:{}".format(crs))

    #set bbox coordinates
    bbox_poly = create_bbox_polygon(bbox=bbox)

    #save record and close shapefile
    rowDict = {
    'geometry' : mapping(bbox_poly), 
    'properties': {'Name' : 'bbox'},
    }
    shp.write(rowDict)
    #close fiona object
    shp.close()

    return output_shape_path

##########################
def delete_shapefile(shapefile_path: str):
    """
    Description:
        little wrapper for deleting a shapefiles files
    
    Args:
        shapefile_path: path to the shapefile to delete

    Return:
        int: 0
    """
    if os.path.exists(shapefile_path):
        base_path = os.path.splitext(shapefile_path)[0]
        for ext in ['.shp','.shx','.dbf','.sbn','.sbx','.fbn','fbx','.prj','.xml','.cpg']:
            file_path = base_path+ext
            if os.path.exists(file_path):
                os.remove(file_path)

    else:
        raise FileNotFoundError('file not found: {}'.format(shapefile_path))

##########################
def copy_shapefile( 
    input_shapefile_path: str,
    output_shapefile_path: str):
    """
    Description:
        copies a shapefile and outputs it to a new location

    Args:
        input_shapefile_path: path to the shapefile to split
        output_shapefile_path: path to output the shapefile too
        add_fid: boolean if True adds an auto generated fid to each feature

    Return:
        int: 0
    """  
    # create output subfolders as needed
    output_dir = os.path.dirname(output_shapefile_path) 
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # Read the original Shapefile
    with fiona.open(input_shapefile_path, 'r') as input:
        # The output has the same schema
        output_schema = input.schema.copy()

        # write a new shapefile
        with fiona.open(
            output_shapefile_path, 
            'w', 
            driver=input.driver,
            crs=input.crs,
            schema=output_schema) as output:
            for feature in input:
                output.write({'properties': feature['properties'],'geometry': mapping(shape(feature['geometry']))})

    return output_shapefile_path

##########################
def check_add_wpid_to_shapefile( 
    input_shapefile_path: str,
    overwrite:bool = False):
    """
    Description:
        checks for and if not present adds a wpid to a 
        shapefile and outputs it to the same location

        wpid: geometry unique identifer (the name is abritrary wpid = waporact id)

    Args:
        input_shapefile_path: path to the shapefile to add fid to as
        organisational index
        overwrite: if true recreates the wpid index even if it is preexisting

    Return:
        int: 0
    """
    add_wpid = False
    # check for fid
    with fiona.open(input_shapefile_path, 'r') as input:
        schema = dict(input.schema)
        if 'wpid' not in schema["properties"].keys():
            add_wpid = True

    if overwrite:
        add_wpid = True

    if add_wpid:
        temp_shapefile_path = os.path.splitext(input_shapefile_path)[0] +'_delete.shp'
        # Read the original Shapefile
        with fiona.open(input_shapefile_path, 'r') as input:
            # The output has the same schema
            output_schema = input.schema.copy()
            output_schema['properties']['wpid'] = 'int'

            # write a new shapefile
            with fiona.open(
                temp_shapefile_path, 
                'w', 
                driver=input.driver,
                crs=input.crs,
                schema=output_schema) as output:
                wpid = 1
                for feature in input:
                    feature['properties']['wpid'] = wpid
                    output.write({'properties': feature['properties'],'geometry': mapping(shape(feature['geometry']))})
                    wpid +=1

        copy_shapefile(
            input_shapefile_path=temp_shapefile_path,
            output_shapefile_path=input_shapefile_path)

        delete_shapefile(shapefile_path=temp_shapefile_path)

    return input_shapefile_path

##########################
def add_matched_values_to_shapefile( 
    input_shapefile_path: str,
    value_type: str,
    new_column_name: str,
    values_dict: dict,
    union_key: str):
    """
    Description:
        adds matching values to a shapefile using a dict. where the keys in the 
        dict are existing values in the shapefile matched to the right column
        using the union_key, the dict value gets inserted in
        
    Args:
        input_shapefile_path: path to the shapefile to add the lcc category names too
        new_column_name: name of the new vlaues column
        value_type: type of the new vlaues being added
        values_dict: dict used to match and add values
        union_key: column in the shapefile used to match and add the values

    Return:
        int: 0
    """
    temp_shapefile_path = os.path.splitext(input_shapefile_path)[0] +'_delete.shp'
    # Read the original Shapefile
    with fiona.open(input_shapefile_path, 'r') as input:
        # The output has the same schema
        output_schema = input.schema.copy()
        output_schema['properties'][new_column_name] = value_type

        # write a new shapefile
        with fiona.open(
            temp_shapefile_path, 
            'w', 
            driver=input.driver,
            crs=input.crs,
            schema=output_schema) as output:
            for feature in input:
                feature['properties'][new_column_name] = values_dict[feature['properties'][union_key]]
                output.write({'properties': feature['properties'],'geometry': mapping(shape(feature['geometry']))})

    copy_shapefile(
        input_shapefile_path=temp_shapefile_path,
        output_shapefile_path=input_shapefile_path)

    delete_shapefile(shapefile_path=temp_shapefile_path)

    return input_shapefile_path

##########################
def check_column_exists( 
    shapefile_path: str,
    column: str):
    """
    Description:
        checks for the existence of a column in the shapefile and if it 
        does not exist raises an error

    Args:
        shapefile_path: path to the shapefile to check
        column: column to check for
    
    Return:
        int: 0

    Raise:
        KeyError: if the column is not found
    """
    with fiona.open(shapefile_path, 'r') as input:
        schema = dict(input.schema)
        if column not in schema["properties"].keys():
            raise KeyError('column: {} not found in shapefile: {} , please provide an exisitng column'.format(column, shapefile_path))

    return 0


####################################################
# spatial operations
####################################################
def create_spatial_index(
    feature_dict: dict,
    id_key: str='wpid'):
    """
    Description:
        create a set of spatial indices for spatial analysis
    
    Args:
        feature_dict: dict to dicts (features) to index
        uses the keys in the top dict as the index value

    Return:
        spatial index object:  
    """
    print('attempting to create spatial indices')
    if not isinstance(feature_dict, dict):
        raise AttributeError('features must be a dictionary')
    # create a spatial index object
    idx = rtree.index.Index()
    # populate the spatial index
    for key in feature_dict:
        geometry = shape(feature_dict[key]['geometry'])
        idx.insert(feature_dict[key]['properties'][id_key], geometry.bounds)

    return idx

###########################
def calc_lat_lon_polygon_area(
    polygon: dict,
    epsg: int=4326):
    """
    Description:
        calculate area in m2 for a polygon
    
    Args:
        polygon: polygon to measure
        epsg: epsg (crs) code to transform from

    Return:
        float: area in m2 of the polygon  
    """
    proj_geom = ops.transform(
        partial(
            pyproj.transform,
            pyproj.Proj('EPSG:{}'.format(epsg)),
            pyproj.Proj(
                proj='aea',
                lat_1=polygon.bounds[1],
                lat_2=polygon.bounds[3]
            )
        ),
        polygon)
    
    return proj_geom.area

###########################
def polygon_area_drop(
    feature_dict: dict, 
    area_threshold: float,
    epsg: int):
    """
    Description:
        sub function used to find and drop features in a feature dict that are
        smaller than the threshold given in m2

        meant to be used as a sub function

    Args:
        feature_dict: fiona based feature dict organised
        by a specific id
        area_threshold: threshold under whioch to drop features
        epsg: epsg (crs) code of the features

    Return: 
        dict: original feature dict with too small features dropped
    """
    print('deleting features/polygons that are smaller than {} m2'.format(area_threshold))
    drop_small_ids = []
    for _id,feature in feature_dict.items():
        poly = shape(feature['geometry'])
        if isinstance(poly, MultiPolygon):
            pass
        if epsg == 4326:
            area = calc_lat_lon_polygon_area(poly)
        else:
            area = poly.area
        if area < area_threshold:
            drop_small_ids.append(_id)

    # drop too small features
    for _id in drop_small_ids:
        feature_dict.pop(_id) 

    return feature_dict


###########################
def fill_small_polygon_holes(
    feature_dict: dict, 
    area_threshold: float,
    epsg: int):
    """
    Description:
        sub function used to find meausre and fill holes in polygons (interor rings)
        that are too small

        meant to be used as a sub function

    Args:
        feature_dict: fiona based feature dict organised
        by a specific id
        area_threshold: threshold under whioch to drop features
        epsg: epsg (crs) code of the features
        id_key: to use for indexes auto set to the package wpid
        expected that the same ids are present in the spatial_indeces

    Return: 
        dict: original feature dict with too small features dropped
    """
    print('filling holes in polygons that are smaller than {} m2'.format(area_threshold))
    for __ , feature in feature_dict.items():
        list_interiors = []
        poly = shape(feature['geometry'])
        if len(poly.interiors) > 0:
            for interior in poly.interiors:
                if epsg == 4326:
                    area = calc_lat_lon_polygon_area(interior)
                else:
                    area = interior.area
                if area > area_threshold:
                    list_interiors.append(interior)
        
            feature['geometry'] = mapping(Polygon(poly.exterior.coords, holes=list_interiors))

    return feature_dict

###########################
def check_for_overlap(
    spatial_indices: rtree.index, 
    feature_dict: dict, 
    feature: dict,
    id_key: str = 'wpid'):
    """
    Description:
        checks if a specific feature overlaps with any other feature
        
        takes a dictionary of features and a matching set of spatial indices and 
        a specific feature and finds if that feature intersects with any other

        meant to be used as a sub function

    Args:
        spatial_indices: rtree based spatial index object
        feature_dict: fiona based feature dict organised
        by a specific id
        feature: fiona based feature to check
        id_key: to use for indexes auto set to the package wpid
        expected that the same ids are present in the spatial_indeces

    Return: 
        list: intersecting features
    """
    intersecting_ids = []
    check_geometry = shape(feature['geometry'])

    existing_ids = [feature_dict[key]['properties'][id_key] for key in feature_dict]

    if id_key not in feature['properties'].keys():
        raise KeyError('required id_key: {} not found among the feature properties while checking for intersections'.format(id_key))

    # get list of ids where bounding boxes intersect
    ids = [int(i) for i in spatial_indices.intersection(check_geometry.bounds)]

    ids = [id for id in ids if id != int(feature['properties'][id_key])]

    ids = [id for id in ids if id in existing_ids]

    # access the features that those ids reference
    for id in ids:
        sub_feature = feature_dict[id]
        sub_geom = shape(sub_feature['geometry'])

        # check the geometries intersect, not just their bboxs
        if check_geometry.overlaps(sub_geom) or check_geometry.crosses(sub_geom) and not check_geometry.touches(sub_geom):
            intersecting_ids.append(id) 

    return intersecting_ids


###########################
def overlap_among_features(
    spatial_indices: rtree.index, 
    feature_dict: dict,
    id_key: str = 'wpid'):
    """
    Description:
        checks if any features overlap, takes a dictionary of features and 
        checks if any of the features intersect

        meant to be used as a sub function

    Args:
        spatial_indices: rtree based spatial index object
        feature_dict: fiona based feature dict organised
        by a specific id
        id_key: to use for indexes auto set to the package wpid
        expected that the same ids are present in the spatial_indices

    Return: 
        bool: True if intersection exists, False if Not
    """
    overlap_exists = False

    existing_ids = [feature_dict[key]['properties'][id_key] for key in feature_dict]

    temp_key = next(iter(feature_dict))
    if id_key not in feature_dict[temp_key]['properties'].keys():
        raise KeyError('required id_key: {} not found among the feature properties while checking for intersections'.format(id_key))

    for key in feature_dict:
        if overlap_exists:
            break
        check_geometry = shape(feature_dict[key]['geometry'])

        # get list of ids where bounding boxes intersect
        ids = [int(i) for i in spatial_indices.intersection(check_geometry.bounds)] # not the same as shapely intersection

        ids = [id for id in ids if id != int(feature_dict[key]['properties'][id_key])] # exclude yourself

        ids = [id for id in ids if id in existing_ids]

        # access the features that those ids reference
        for id in ids:
            sub_feature = feature_dict[id]
            sub_geom = shape(sub_feature['geometry'])

            # check the geometries intersect, not just their bboxs
            if check_geometry.overlaps(sub_geom) or check_geometry.crosses(sub_geom) and not check_geometry.touches(sub_geom):
                overlap_exists = True
                break

    return overlap_exists

##########################
def union_and_drop(
    spatial_indices: rtree.index, 
    feature_dict: dict, 
    id_key: str = 'wpid'):
    """
    Description:
        takes a colleciton of features and a matching set of spatial indices and 
        unions applicable features and drops the ones unioned from the dict

        meant to be used as a sub function

    Args:
        spatial_indices: rtree based spatial index object
        feature_dict: fiona based feature dict organised
        by a specific id
        id_key: to use for indexes auto set to the package wpid
        expected that the same ids are present in the spatial_indeces

    Return: 
        list: unioned and filtered features
    """
    checked_ids = []
    drop_ids =[]

    # reset index for security
    feature_dict = {feature_dict[key]['properties'][id_key]: feature_dict[key] for key in feature_dict}

    # edit the polygons
    for _id, feature in feature_dict.items():
        # check for intersections and union
        if _id not in checked_ids:
            overlapping_ids = check_for_overlap(
                spatial_indices=spatial_indices, 
                feature_dict=feature_dict, 
                feature=feature,
                id_key=id_key)

            checked_ids.append(_id)
            overlapping_ids = [_id for _id in overlapping_ids if _id not in checked_ids]

            if overlapping_ids:
                # list geometries that overlap
                union_geoms = [(_id,shape(feature_dict[_id]['geometry'])) for _id in overlapping_ids]
                union_geoms.append((_id, shape(feature_dict[_id]['geometry'])))
                # calculate largest geometry to maintain most relevant assumed) properties
                union_geoms = [(geom[0], geom[1], geom[1].area) for geom in union_geoms]
                # sort by size
                union_geoms.sort(key=lambda x:x[2])
                # union geoms
                unioned_geom = ops.unary_union([geom[1] for geom in union_geoms])
                # replace existing geometry
                feature_dict[_id]['geometry'] = mapping(unioned_geom)
                # replace existing properties with that of the largest geom
                feature_dict[_id]['properties'] = feature_dict[unioned_geom[0][0]]['properties']
                # reset replaced id back to current id
                feature_dict[_id]['properties'][id_key] = _id
                drop_ids.extend(overlapping_ids)  
                checked_ids.extend(overlapping_ids)

    # drop unionised features
    for _id in drop_ids:
        feature_dict.pop(_id)

    return feature_dict

##########################
def polygonize_cleanup(
    input_shapefile_path: str,
    output_shapefile_path: str,
    area_threshold: float = 0,
    id_key: str = 'wpid'):
    """
    Description:
        takes a set of polygons and cleans them up 
        with the holes filled and edges smoothed out

        built to be used on the output of gdal polygonize

        NOTE: this is a simple function, more complicated procedures 
        are possible but if you want them you will have to look into it
        yourself

    Args:
        input_shapefile_path: path to the shapefile to split
        output_shapefile_path: path to output the celaned shapefile too
        fill_holes: if True fills the holes in seperate polygons found
        using a buffer around the exterior
        round_edges: if true rounds the edges of the polygons smoothing out 
        the polygons
        area_threshold: filters out polygons with an area smaller than this 
        threshold in the input raster (in m2)
        id_key: id used to organise the features

    Return:
        int: 0
    
    """
    # create output subfolders as needed
    output_dir = os.path.dirname(output_shapefile_path) 
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    if id_key == 'wpid':
        # check for and add possible missing identifier:
        check_add_wpid_to_shapefile(input_shapefile_path=input_shapefile_path)

    # retrieve feature_dict from the input file
    with fiona.open(input_shapefile_path, 'r') as input:
        output_driver = input.driver
        output_crs = input.crs
        output_schema = dict(input.schema) 
        output_schema['geometry'] = "Polygon"

        features = {feature['properties'][id_key]: feature for feature in input}

        epsg_code = int(input.crs['init'].replace('epsg:',''))

    input = None

    print('attempting to clean up {} polygon features'.format(len(features)))

    # fill the holes that are too small
    features = fill_small_polygon_holes(
        feature_dict=features, 
        area_threshold=area_threshold,
        epsg=epsg_code)

    # remove the too small polygons premeptively to speed up the process
    features = polygon_area_drop(
        feature_dict=features, 
        area_threshold=area_threshold,
        epsg=epsg_code)
        
    print('smallest features dropped {} polygon features remaining'.format(len(features)))

    # create spatial index
    idx = create_spatial_index(feature_dict=features, id_key=id_key)

    prev_count = 0
    current_count = 0
    no_change_in_count = 0

    while overlap_among_features(
        spatial_indices= idx, 
        feature_dict= features):

        # union features
        features = union_and_drop(
            spatial_indices=idx, 
            feature_dict=features)

        # fill the holes that are too small
        fill_small_polygon_holes(
            feature_dict=features, 
            area_threshold=area_threshold,
            epsg=epsg_code)

        features= polygon_area_drop(
            feature_dict=features, 
            area_threshold=area_threshold,
            epsg=epsg_code)
       
        print('smallest features dropped {} polygon features remaining'.format(len(features)))

        print('polygon features unionised: {} remaining'.format(len(features)))
        current_count = len(features)
        if prev_count == current_count:
            no_change_in_count +=1

        if no_change_in_count >= 5:
            print('WARNING: features no longer unionising but still overlapping,'
            ' breaking loop and moving on but be aware that polygons still overlap')
            break
        else:
            prev_count = current_count 

            # recreate spatial index
            idx = create_spatial_index(feature_dict=features, id_key=id_key)

    # write the edited polygons to file
    with fiona.open(
        output_shapefile_path, 
        'w', 
        driver=output_driver,
        crs=output_crs,
        schema=output_schema) as output:
    
        # write the input file to output
        for fid, feature in features.items():
            poly = shape(feature['geometry'])
            if isinstance(poly, Polygon):
                    output.write({
                        'properties': feature['properties'],
                        'geometry': mapping(poly)
                    })
            elif isinstance(poly, MultiPolygon):
                for subpoly in poly:
                    output.write({
                        'properties': feature['properties'],
                        'geometry': mapping(subpoly)
                    })

            else:
                print('non polygon feature found at the end of cleaning up polygons ... discarding geometry')
                pass

    output = None

    # check for and add possible missing identifier:
    check_add_wpid_to_shapefile(input_shapefile_path=output_shapefile_path, overwrite=True)

    return output_shapefile_path

##########################
def raster_to_polygon(
    input_raster_path: str,
    output_shapefile_path: str,
    column_name: str = 'value',
    column_type = ogr.OFTInteger,
    mask_raster_path : str = None):
    """
    Description:
        internal polygonize method built around gdal polygonize that 
        polygonizes/vectorises cells in a raster

    Args:
        input_raster_path: path to the raster to polygonize
        output_shapefile_path: path to output the made shapefile too
        mask_raster_path: if provided masks to that raster
        column_name: name for the shapefile column that contains the polygonized 
        raster value
        column_type: ogr value type for setting the type fo the value, gnenerally:
        ogr.OFTInteger or ogr.OFTReal

    Return:
        int: 0
    """
    # create output subfolders as needed
    output_dir = os.path.dirname(output_shapefile_path) 
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    source_dataset = gdal.Open(input_raster_path)
    if source_dataset is None:
        raise AttributeError('Unable to open %s' % source_dataset)

    source_band = source_dataset.GetRasterBand(1)
    
    if mask_raster_path:
        mask_dataset = gdal.Open(mask_raster_path)
        if mask_dataset is None:
            raise AttributeError('Unable to open %s' % mask_dataset)
        mask_band = mask_dataset.GetRasterBand(1)
    else:
        mask_band = None

    shapefile_name = os.path.splitext(os.path.basename(output_shapefile_path))[0]
    drv = ogr.GetDriverByName("ESRI Shapefile")
    shapefile_out = drv.CreateDataSource(output_shapefile_path)

    # create the spatial reference, WGS84
    srs = osr.SpatialReference()
    srs.ImportFromEPSG(gdal_info(input_raster_path)['crs'])

    # create the layer
    shapefile_layer = shapefile_out.CreateLayer(shapefile_name, srs, geom_type=ogr.wkbPolygon)
    
    newField = ogr.FieldDefn(column_name, column_type)
    shapefile_layer.CreateField(newField)
    gdal.Polygonize(source_band, mask_band, shapefile_layer, 0, [])

    shapefile_out = shapefile_layer = None

    # check for and add possible misisng identifier:
    check_add_wpid_to_shapefile(input_shapefile_path=output_shapefile_path)

    return output_shapefile_path
 
if __name__ == "__main__":
    start = default_timer()
    args = sys.argv
